---
slug: ai-praktijk-handelingskaders-rbak-pbak
title: 'AI en Praktijk: Handelingskaders en toepassing'
authors: [ruben]
tags: [ai, overheid, privacy]
---

# AI en Praktijk: Handelingskaders en toepassing

:::warning Status: Concept
Deze blog is nog in concept-fase en wordt mogelijk nog aangepast voor publicatie.
:::

AI kan de overheid efficiënter en menselijker maken, maar alleen als we het streng begrenzen met duidelijke handelingskaders. Stel je voor dat AI adviezen geeft voor subsidies of vergunningen – nuttig, maar hoe zorg je dat het binnen juridische en ethische grenzen blijft? Dit blog duikt in praktische toepassing, met voorbeelden, en scheidt wat AI mag (voorbereiden, adviseren) van wat niet (beslissen). Voor technische integratie, zie ons blog "AI en Techniek: Veilige integratie".

<!--truncate-->

## De Uitdaging: AI Binnen Strakke Grenzen Houden

AI zonder grenzen is gevaarlijk – het kan biases versterken, privacy schenden of besluiten nemen die niemand begrijpt. De kernvraag is: wanneer zet je AI in, en wanneer niet? In de praktijk betekent dit een zorgvuldige balans tussen ondersteuning en controle, met altijd de mens als eindverantwoordelijke. Want laten we eerlijk zijn, AI is slim, maar niet slim genoeg om je baas te zijn – de mens moet altijd de finale call maken.

Zonder grenzen risico's: oncontroleerbare adviezen, privacy-lekken (zoals in Eindhoven waar ambtenaren residentdata lekten via public AI-tools), juridische problemen (boetes onder AVG of AI Act). Met grenzen: transparantie, controle, vertrouwen.

## Wanneer AI Wel Inzetten: Ondersteunende Rollen met Praktijkvoorbeelden

AI is geschikt voor taken die ondersteunen, versnellen of verhelderen – altijd met menselijke controle. Neem een praktijkvoorbeeld uit een Nederlandse gemeente: AI wordt ingezet om lange dossiers samen te vatten voor ambtenaren die subsidieaanvragen beoordelen. In plaats van uren ploegen door pagina's, krijgt de ambtenaar een kernsamenvatting: "Burger voldoet aan criteria A en B, maar check C wegens mogelijke uitzondering." De ambtenaar controleert en beslist – AI versnelt het proces, reduceert fouten, en maakt het werk leuker, zonder de verantwoordelijkheid over te nemen.

Een ander voorbeeld: bij fraude-signalering in uitkeringen helpt AI patronen te spotten in geanonimiseerde data, zoals onregelmatige inkomstenstromen. Maar in plaats van automatisch te flaggen, geeft het een advies: "Dit patroon wijst op mogelijk onjuiste opgave – aanbevolen: extra verificatie." De ambtenaar onderzoekt – dit voorkomt discriminatie, zoals in gevallen waar algoritmes bepaalde groepen onevenredig raken, en houdt het ethisch verantwoord.

Of bij burgercontact: AI structureert inkomende emails, extracteert sleutelinfo zoals naam en urgentie, en routeert naar de juiste afdeling. In pilots bij gemeenten verkort dit de responstijd van dagen naar uren – een grapje waard: AI is de perfecte secretaresse, maar zonder koffie te morsen. Inzet wanneer: laag risico op fouten met impact, hoog waarde in snelheid/accuratesse, mens controleert output.

## Wanneer AI Niet Inzetten: Kritieke Besluiten en Autonome Acties

AI niet inzetten voor autonome besluiten of acties met directe impact – te riskant, juridisch onmogelijk. Neem het voorbeeld van SyRI, het Nederlandse systeem voor fraudeopsporing dat verboden werd door de rechter: het profileerde burgers te autonoom, zonder voldoende transparantie, en leidde tot mogelijke discriminatie. Les: geen AI die zelfstandig subsidies toekent, boetes oplegt of zorg toewijst – dat vereist menselijk oordeel voor nuances en verantwoordelijkheid.

Een ander praktijkvoorbeeld: bij persoonlijke situaties zoals uitkeringsbesluiten in gemeenten, waar AI wel patronen kan signaleren, maar niet mag beslissen – risico op fouten in complexe gevallen, zoals medische uitzonderingen, is te groot. Het zou een slechte grap zijn als AI je uitkering afwijst omdat het je kat als 'afhankelijk familielid' ziet. Niet inzetten wanneer: high-risk (EU AI Act), directe rechten raken, onvoorspelbaarheid leidt tot fouten, privacy-lekken of juridische problemen – zoals in het Clearview AI-geval, waar het bedrijf €30 miljoen boete kreeg van de Nederlandse AP voor illegale biometrische data-verwerking, of de toename van databreuken door AI-chatbots als ChatGPT in werkomgevingen, waaraan de AP waarschuwt.

## Juridische Duiding: Het Handelingskader van AI

Juridisch gezien mag AI in overheid geen handelingsbevoegdheid hebben – besluiten moeten herleidbaar zijn tot een menselijke ambtenaar (artikel 1:3 Awb en artikel 3:2 Awb). AI is tool, geen actor; het mag adviseren of voorbereiden, maar nooit bindende beslissingen nemen, rechten toekennen/ontzeggen of plichten opleggen. Dit voorkomt aansprakelijkheidskwesties en waarborgt democratie – mens blijft accountable. In EU AI Act high-risk categorieën vereist dit expliciete menselijke oversight.

## AI als Ondersteunende Tool: Samenvatten, Genereren, Structureren – Met Menselijke Controle

AI mag in overheid ondersteunen, niet leiden. Het kan lange dossiers samenvatten tot kernpunten – denk aan een 200-pagina rapport dat AI reduceert tot een overzichtelijke twee pagina's met highlights, zodat de ambtenaar snel de essentie grijpt en details controleert. Of conceptbrieven en adviezen genereren: AI draft een standaardbrief voor kwijtschelding, met placeholders voor persoonlijke info – ambtenaar past aan, voegt nuance toe, verstuurt. En ongestructureerde data structureren: een warrige burgeremail vol vragen wordt door AI geparsed – naam, BSN, urgentie eruit gehaald – en gerouteerd naar de juiste afdeling.

Maar altijd met menselijke controle: ambtenaar checkt output, past aan, beslist – AI versnelt, mens waarborgt accuraatheid en ethiek. Rode lijn: nooit autonoom – geen besluiten, overboekingen, wijzigingen. Altijd menselijke controle.

## Human in the Loop: De Mens Blijft Centraal

Human in the Loop (HITL) is cruciaal: menselijke oversight in AI-processen. In overheid betekent dit: AI adviseert of prepareert, maar mens controleert, beslist. Voorbeeld: AI genereert subsidie-advies, ambtenaar valideert – voorkomt fouten, biases, waarborgt ethiek. HITL houdt AI accountable, past bij democratische waarden en AI Act.

## Ethische Handelingsbevoegdheid: Mens Centraal

Techniek alleen volstaat niet – ethiek is key. Verantwoord: AI versnelt werk, voorkomt tikfouten of vergeten checks, maakt informatie toegankelijker – mens eindverantwoordelijk.

Problematisch: AI vervangt oordeel, discrimineert (zelfs als "statistisch klopt"), creëert machteloosheid.

Voorbeeld SyRI: verboden door black box, discriminatie-risico, disproportioneel. Les: transparant, uitlegbaar, proportioneel.

Kaders: transparantie (burger weet AI-gebruik), proportionaliteit, menselijke controle, non-discriminatie (bias-testing).

## Kritische Vraag: Vergroten We de Kloof Als We AI Niet Inzetten?

Als we AI niet inzetten om burgers door regeldoolhoven te helpen, profiteren alleen digitaal vaardigen – vergroten we de kloof niet?

Antwoord: ja, daarom wél inzetten – verantwoord, binnen kaders, voor iedereen toegankelijk.

## Conclusie

Goed ingerichte AI vergroot bestaanszekerheid, niet bureaucratie. Praktische waarborgen, juridische kaders (bevoegdheid, transparantie), ethische grenzen (mens beslist, non-discriminatie).

AI voorbereidt en adviseert. Mens beslist.

Met waarborgen maken we overheid toegankelijker, effectiever, menselijker – zonder concessies aan veiligheid, privacy, democratie.

## Bronnen

- [^1]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^2]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^3]: **EU AI Act** - [High-risk AI en transparantie](https://artificialintelligenceact.eu/)
- [^4]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^5]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^6]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^7]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^8]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^9]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^10]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^11]: **Autoriteit Persoonsgegevens** - [Boete Clearview AI voor illegaal verwerken van biometrische gegevens](https://autoriteitpersoonsgegevens.nl/nl/nieuws/ap-boete-clearview-ai-voor-illegaal-verwerken-van-biometrische-gegevens)
- [^12]: **Rijksoverheid** - [Algemene wet bestuursrecht (Awb)](https://wetten.overheid.nl/BWBR0005537/2025-01-01)
- [^13]: **EU AI Act** - [High-risk AI en transparantie](https://artificialintelligenceact.eu/)
- [^14]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^15]: **Waag** - [Open AI en ethiek](https://waag.org/)
- [^16]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^17]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^18]: **Rathenau Instituut** - Ethische AI en SyRI-analyse: diverse rapporten
- [^19]: **Rijksoverheid** - [Visie generatieve AI en handelingskaders](https://open.overheid.nl/)
- [^20]: **VNG** - [AI in gemeentelijke praktijk](https://vng.nl/)
- [^21]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^22]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
- [^23]: **iBestuur** - [Artikelen over AI-kaders in overheid](https://ibestuur.nl/)
