---
slug: ai-bubbels-wereldbeeld
title: De AI die het beste bij jouw bubbel past - Hoe werkt AI eigenlijk?
authors: [ruben]
tags: [ai, platform-ai, overheid, ethiek, technologie, llm]
---

# De AI die het beste bij jouw bubbel past - Hoe werkt AI eigenlijk?

:::warning Status: Concept
Deze blog is nog in concept-fase en wordt mogelijk nog aangepast voor publicatie.
:::

We krijgen vaak de vraag: welke AI moet ik kiezen? Mistral, Cursor, ChatGPT? Wat is het meest ethische en wat is het beste?

Een logische vraag, maar het antwoord is grappig: **Kies de AI die bij je social bubbel past.** huh die bubbels weer? Ja AI's volgen net zo goed sociaal media bubbels, of althans ze zijn onderheving aan "waarden, normen en wereldbeelden" aan de hand van de informatie die ze tot zich nemen of juist negeren. In die zin zijn ze niet heel anders dan een social media gebruiker. Maar om te begrijpen hoe dit werkt (en waarom je er op moet meer rollen) moeten we eerst even duiken in wat AI eigenlijk is.

## Wat is AI Eigenlijk?

Laten we het mysterie ontrafelen: **AI is geen magie, maar een tekstvoorspeller**.

Een LLM (Large Language Model) is een rekenmodel dat getraind is op enorme hoeveelheden tekst. Het heeft patronen geleerd uit die voorbeelden. Wanneer je iets invoert, voorspelt het model stap voor stap wat de meest logische volgende tekst zou zijn - puur op basis van waarschijnlijkheden en patronen en (met name) de gestelde vraag. 

 AI **denkt niet na** zoals een mens. Het geeft simpelweg de meest waarschijnlijke volgende tekst terug op basis van wat het heeft geleerd. En dus die gestelde vraag. Met name dat laatste maakt het zo fijn om mee te werken. Het geeft in het ware het meest wensenlijke antwoord op je vraag en is dus een zekere zin een digitale pschychopaat. En dat levert ook een acuut probleem op


## AI is net zo Ethisch als de Data

Hier wordt het interessant: **AI heeft per definitie geen ethisch, moreel of sociaal wegingskader**. Het voorspelt alleen maar op basis van de data die het heeft gezien. Met andere woorden Training = Opvoeding = Gedrag.

Het trainen van AI betekent dat je het model voedt met voorbeelden en data, zodat het daar patronen in kan herkennen. Je laat het model als het ware oefenen, zodat het steeds beter wordt in voorspellen. De antwoorden zijn dus net zo ethisch of moreel als de data waarmee je het traint. En daar ligt de kern van het probleem, op welke data is een model getrained.

### De Valkuil van het Internet

Als je AI traint op het hele internet of Twitter, of een klasieker voorbeeld de comment sectie vann nu.nl dan neemt het ook alle vooroordelen, ruis en minder fraaie kanten over. Het creert immers een voorspeleng patroon voor dat soort comments. De bron van je trainingsdata bepaalt de bias van je AI.

Ethiek betekent dus: zorgen dat de patronen waar AI uit leert komen uit een zorgvuldig samengestelde, moreel verantwoorde dataset. Maar dat creert grappig genoeg weer andere problemen. 

## There are three kinds of lies: lies, damned lies, and statistics.

Ook in wetenschapenlijk verband is het maar net de vraag welke data je kiest en in welke ccontext die je plaats, er zijn genoeg cijvers te vinden waar afkomst (het zij etnisch of economisch) aan toekomst perspectief of crimineel gedrgadd kan worden gekopeld. En zelf dan nog is er een reden dat we een tweede kamer puur bestaande uit hoogopgeleide blanken mannen onwensenlijk vinden. Het mist gevoel met de realiteit, en het wetenschapenlijk of economisch meest wijze is niet per definitie het sociaalse of wensenlijkste. Sterker nog als we puur sturen op wetenschappelijke data en statistische eerlijkheid, kan AI conclusies trekken die statistisch correct zijn maar sociaal onwenselijk. Wat statistisch "eerlijk" lijkt, kan in de praktijk discriminerend aanvoelen.

Een tweede probleem met "etische" ai's is kennis deficiet, heel flauw maar het uitslutien van auterisrechtenlijk beschemrd matriaal leid tot minder kennis. En is hoewel normatief goed the verantwoorden gek vanuit het concept AI als asistent. Een mens put ook uit alle kennis die hij ooit is tegen gekomen zonder te wegen of dit uit een bepaald boek of artikel kwam.  

Maar goed daarmee komen we op een fundamenteel probleem, de neutrale AI bestaad net zo min als de neutrale mens. Het levert per definitie een gekleurd beeld op aan de hand keuzes van de trainer. Daarbij is een AI getrained op wetenschapenlijke artikelen net zo min etisch als een getraing op de onderstroom van twitten. Maar wat moeten we dan wel?  

## Hou op met het jagen op eenhoors

Een perfecte AI ga je niet vinden, iedere filisoof had je dat op voorhand kunnen vertellen. Wat je ook traind en hoe je het in elkaar schroeft als de data het geheel van de mensheid is dan krijg je iets mensenljks. De vraag is hoe je hiermee omgaat. En dat is eigenlijk verdomt simpel we doen het al jaren. Iedere ambtelijke organisatie bestaad uit menenlijke mensen die we ooit hebben aangenomen in de volste overtuiging dat ze nooit 100% neutraal zijn. En daarop hebben we toesting, bezwaar en een legio aan beheers maatregelen genomen.

Los daarvan denkt AI in patronen, niet in feitenlijk data. Als veel teksten naar een wetstekst verwijzen dan is dat een patroon, maar als ergens een wet of precedent logisch in het patroon past (doch niet feitenlijk bestaad) dan wordt het een halucinatie. Een AI die er van overtuigs is dat weten of precedenten bestaan inclusief bronvermelding zonder dat ze er daadwerkenljik zijn. Zelf puur trainen op waarheid voorkomt dus nog geen onwaarheden.

Waar we met AI dus op moeten letten is zorgen dat we advies niet als neutraal ervaren, niet klakkenloos overnemen en niet als absolute waarheid zien. 

## Dus weer even terug naar de Bubbel

En zo komen we terug bij waar we begonnen: AI leeft in een bubbel, waar kunnen we dan wel op letten bij inhoudenlijke keuzes voor een AI model? Daar is eigenlijk iets heel simpels over te zeggen, we willen dat zijn bubbel een beetje match bij de onze. 

- We willen vertrouwen hebben trainers van de AI
- We willen inzicht hebben de trainigsdata van de AI
- We willen dat de AI een brede set heeft aan trainings data (niet alleen nu.nl niet alleen wetenschappenlijk)
- We willen de AI locaal kunnen draaien

(dat laatste is trouwens cruciaal om gigantische data lekken te voorkomen maar daar hebben we het een andere keer verder over) het zou natuurlijk helpen als het model zelf ook open source is zodat we het kunnen testen in gedrag. Oke dan komen we op het volgende uit.

[tabal van veel gebruikte moddele invoegen]

Het belangrijkte is echter dat we ons bewust zijn van deze randjes van AI, we kunnen AI er nooit meer op vertrouwen om zelfstandig namens de overheid te opereren in casusen zo als zaak orgestratie etc. Het gaat in die zin nooit de inhoudenlijk medewerker vervangen. Waar het echter wel een wereld van verschil kan maken is de snelheid waarmee het data doorzoekt en voorsteld aan die inhouden medewerker. Daarover meer in het volgende blog.

**Volgende blog in deze serie:** AI en Open Source - Waarom transparantie geen luxe is

