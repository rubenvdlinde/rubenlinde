"use strict";(globalThis.webpackChunkrubenlinde_website=globalThis.webpackChunkrubenlinde_website||[]).push([[1621],{2954(e){e.exports=JSON.parse('{"permalink":"/en/blog/ai-praktijk-handelingskaders-rbak-pbak","source":"@site/blog/platform-ai-en-overheid/2025-01-19-ai-praktijk-handelingskaders.md","title":"AI en Praktijk: Handelingskaders en toepassing","description":"Deze blog is nog in concept-fase en wordt mogelijk nog aangepast voor publicatie.","date":"2025-01-19T00:00:00.000Z","tags":[{"inline":true,"label":"ai","permalink":"/en/blog/tags/ai"},{"inline":true,"label":"beveiliging","permalink":"/en/blog/tags/beveiliging"},{"inline":true,"label":"handelingskaders","permalink":"/en/blog/tags/handelingskaders"},{"inline":true,"label":"platform-ai","permalink":"/en/blog/tags/platform-ai"},{"inline":true,"label":"overheid","permalink":"/en/blog/tags/overheid"},{"inline":true,"label":"ethiek","permalink":"/en/blog/tags/ethiek"},{"inline":true,"label":"privacy","permalink":"/en/blog/tags/privacy"}],"readingTime":6.18,"hasTruncateMarker":true,"authors":[{"name":"Ruben van de Linde","title":"Software Developer","url":"https://github.com/rubenvdlinde","email":"your-email@example.com","imageURL":"https://github.com/rubenvdlinde.png","key":"ruben","page":null}],"frontMatter":{"slug":"ai-praktijk-handelingskaders-rbak-pbak","title":"AI en Praktijk: Handelingskaders en toepassing","authors":["ruben"],"tags":["ai","beveiliging","handelingskaders","platform-ai","overheid","ethiek","privacy"]},"unlisted":false,"prevItem":{"title":"AI Kansen voor Burgers - De Regelgeving Navigator","permalink":"/en/blog/ai-kansen-burgers-regelgeving-navigator"},"nextItem":{"title":"AI en Techniek - Veilige integratie en het Mistral-vraagstuk","permalink":"/en/blog/ai-techniek-integratie-mcp-mistral"}}')},8453(e,n,i){i.d(n,{R:()=>o,x:()=>s});var t=i(6540);const r={},a=t.createContext(r);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:n},e.children)}},9040(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>t,toc:()=>d});var t=i(2954),r=i(4848),a=i(8453);const o={slug:"ai-praktijk-handelingskaders-rbak-pbak",title:"AI en Praktijk: Handelingskaders en toepassing",authors:["ruben"],tags:["ai","beveiliging","handelingskaders","platform-ai","overheid","ethiek","privacy"]},s="AI en Praktijk: Handelingskaders en toepassing",l={authorsImageUrls:[void 0]},d=[{value:"De Uitdaging: AI Binnen Strakke Grenzen Houden",id:"de-uitdaging-ai-binnen-strakke-grenzen-houden",level:2},{value:"Wanneer AI Wel Inzetten: Ondersteunende Rollen met Praktijkvoorbeelden",id:"wanneer-ai-wel-inzetten-ondersteunende-rollen-met-praktijkvoorbeelden",level:2},{value:"Wanneer AI Niet Inzetten: Kritieke Besluiten en Autonome Acties",id:"wanneer-ai-niet-inzetten-kritieke-besluiten-en-autonome-acties",level:2},{value:"Juridische Duiding: Het Handelingskader van AI",id:"juridische-duiding-het-handelingskader-van-ai",level:2},{value:"AI als Ondersteunende Tool: Samenvatten, Genereren, Structureren \u2013 Met Menselijke Controle",id:"ai-als-ondersteunende-tool-samenvatten-genereren-structureren--met-menselijke-controle",level:2},{value:"Human in the Loop: De Mens Blijft Centraal",id:"human-in-the-loop-de-mens-blijft-centraal",level:2},{value:"Ethische Handelingsbevoegdheid: Mens Centraal",id:"ethische-handelingsbevoegdheid-mens-centraal",level:2},{value:"Kritische Vraag: Vergroten We de Kloof Als We AI Niet Inzetten?",id:"kritische-vraag-vergroten-we-de-kloof-als-we-ai-niet-inzetten",level:2},{value:"Conclusie",id:"conclusie",level:2},{value:"Bronnen",id:"bronnen",level:2}];function g(e){const n={admonition:"admonition",h2:"h2",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.admonition,{title:"Status: Concept",type:"warning",children:(0,r.jsx)(n.p,{children:"Deze blog is nog in concept-fase en wordt mogelijk nog aangepast voor publicatie."})}),"\n",(0,r.jsx)(n.p,{children:'AI kan de overheid effici\xebnter en menselijker maken, maar alleen als we het streng begrenzen met duidelijke handelingskaders. Stel je voor dat AI adviezen geeft voor subsidies of vergunningen \u2013 nuttig, maar hoe zorg je dat het binnen juridische en ethische grenzen blijft? Dit blog duikt in praktische toepassing, met voorbeelden, en scheidt wat AI mag (voorbereiden, adviseren) van wat niet (beslissen). Voor technische integratie, zie ons blog "AI en Techniek: Veilige integratie".'}),"\n",(0,r.jsx)(n.h2,{id:"de-uitdaging-ai-binnen-strakke-grenzen-houden",children:"De Uitdaging: AI Binnen Strakke Grenzen Houden"}),"\n",(0,r.jsx)(n.p,{children:"AI zonder grenzen is gevaarlijk \u2013 het kan biases versterken, privacy schenden of besluiten nemen die niemand begrijpt. De kernvraag is: wanneer zet je AI in, en wanneer niet? In de praktijk betekent dit een zorgvuldige balans tussen ondersteuning en controle, met altijd de mens als eindverantwoordelijke. Want laten we eerlijk zijn, AI is slim, maar niet slim genoeg om je baas te zijn \u2013 de mens moet altijd de finale call maken."}),"\n",(0,r.jsx)(n.p,{children:"Zonder grenzen risico's: oncontroleerbare adviezen, privacy-lekken (zoals in Eindhoven waar ambtenaren residentdata lekten via public AI-tools), juridische problemen (boetes onder AVG of AI Act). Met grenzen: transparantie, controle, vertrouwen."}),"\n",(0,r.jsx)(n.h2,{id:"wanneer-ai-wel-inzetten-ondersteunende-rollen-met-praktijkvoorbeelden",children:"Wanneer AI Wel Inzetten: Ondersteunende Rollen met Praktijkvoorbeelden"}),"\n",(0,r.jsx)(n.p,{children:'AI is geschikt voor taken die ondersteunen, versnellen of verhelderen \u2013 altijd met menselijke controle. Neem een praktijkvoorbeeld uit een Nederlandse gemeente: AI wordt ingezet om lange dossiers samen te vatten voor ambtenaren die subsidieaanvragen beoordelen. In plaats van uren ploegen door pagina\'s, krijgt de ambtenaar een kernsamenvatting: "Burger voldoet aan criteria A en B, maar check C wegens mogelijke uitzondering." De ambtenaar controleert en beslist \u2013 AI versnelt het proces, reduceert fouten, en maakt het werk leuker, zonder de verantwoordelijkheid over te nemen.'}),"\n",(0,r.jsx)(n.p,{children:'Een ander voorbeeld: bij fraude-signalering in uitkeringen helpt AI patronen te spotten in geanonimiseerde data, zoals onregelmatige inkomstenstromen. Maar in plaats van automatisch te flaggen, geeft het een advies: "Dit patroon wijst op mogelijk onjuiste opgave \u2013 aanbevolen: extra verificatie." De ambtenaar onderzoekt \u2013 dit voorkomt discriminatie, zoals in gevallen waar algoritmes bepaalde groepen onevenredig raken, en houdt het ethisch verantwoord.'}),"\n",(0,r.jsx)(n.p,{children:"Of bij burgercontact: AI structureert inkomende emails, extracteert sleutelinfo zoals naam en urgentie, en routeert naar de juiste afdeling. In pilots bij gemeenten verkort dit de responstijd van dagen naar uren \u2013 een grapje waard: AI is de perfecte secretaresse, maar zonder koffie te morsen. Inzet wanneer: laag risico op fouten met impact, hoog waarde in snelheid/accuratesse, mens controleert output."}),"\n",(0,r.jsx)(n.h2,{id:"wanneer-ai-niet-inzetten-kritieke-besluiten-en-autonome-acties",children:"Wanneer AI Niet Inzetten: Kritieke Besluiten en Autonome Acties"}),"\n",(0,r.jsx)(n.p,{children:"AI niet inzetten voor autonome besluiten of acties met directe impact \u2013 te riskant, juridisch onmogelijk. Neem het voorbeeld van SyRI, het Nederlandse systeem voor fraudeopsporing dat verboden werd door de rechter: het profileerde burgers te autonoom, zonder voldoende transparantie, en leidde tot mogelijke discriminatie. Les: geen AI die zelfstandig subsidies toekent, boetes oplegt of zorg toewijst \u2013 dat vereist menselijk oordeel voor nuances en verantwoordelijkheid."}),"\n",(0,r.jsx)(n.p,{children:"Een ander praktijkvoorbeeld: bij persoonlijke situaties zoals uitkeringsbesluiten in gemeenten, waar AI wel patronen kan signaleren, maar niet mag beslissen \u2013 risico op fouten in complexe gevallen, zoals medische uitzonderingen, is te groot. Het zou een slechte grap zijn als AI je uitkering afwijst omdat het je kat als 'afhankelijk familielid' ziet. Niet inzetten wanneer: high-risk (EU AI Act), directe rechten raken, onvoorspelbaarheid leidt tot fouten, privacy-lekken of juridische problemen \u2013 zoals in het Clearview AI-geval, waar het bedrijf \u20ac30 miljoen boete kreeg van de Nederlandse AP voor illegale biometrische data-verwerking, of de toename van databreuken door AI-chatbots als ChatGPT in werkomgevingen, waaraan de AP waarschuwt."}),"\n",(0,r.jsx)(n.h2,{id:"juridische-duiding-het-handelingskader-van-ai",children:"Juridische Duiding: Het Handelingskader van AI"}),"\n",(0,r.jsx)(n.p,{children:"Juridisch gezien mag AI in overheid geen handelingsbevoegdheid hebben \u2013 besluiten moeten herleidbaar zijn tot een menselijke ambtenaar (artikel 1:3 Awb en artikel 3:2 Awb). AI is tool, geen actor; het mag adviseren of voorbereiden, maar nooit bindende beslissingen nemen, rechten toekennen/ontzeggen of plichten opleggen. Dit voorkomt aansprakelijkheidskwesties en waarborgt democratie \u2013 mens blijft accountable. In EU AI Act high-risk categorie\xebn vereist dit expliciete menselijke oversight."}),"\n",(0,r.jsx)(n.h2,{id:"ai-als-ondersteunende-tool-samenvatten-genereren-structureren--met-menselijke-controle",children:"AI als Ondersteunende Tool: Samenvatten, Genereren, Structureren \u2013 Met Menselijke Controle"}),"\n",(0,r.jsx)(n.p,{children:"AI mag in overheid ondersteunen, niet leiden. Het kan lange dossiers samenvatten tot kernpunten \u2013 denk aan een 200-pagina rapport dat AI reduceert tot een overzichtelijke twee pagina's met highlights, zodat de ambtenaar snel de essentie grijpt en details controleert. Of conceptbrieven en adviezen genereren: AI draft een standaardbrief voor kwijtschelding, met placeholders voor persoonlijke info \u2013 ambtenaar past aan, voegt nuance toe, verstuurt. En ongestructureerde data structureren: een warrige burgeremail vol vragen wordt door AI geparsed \u2013 naam, BSN, urgentie eruit gehaald \u2013 en gerouteerd naar de juiste afdeling."}),"\n",(0,r.jsx)(n.p,{children:"Maar altijd met menselijke controle: ambtenaar checkt output, past aan, beslist \u2013 AI versnelt, mens waarborgt accuraatheid en ethiek. Rode lijn: nooit autonoom \u2013 geen besluiten, overboekingen, wijzigingen. Altijd menselijke controle."}),"\n",(0,r.jsx)(n.h2,{id:"human-in-the-loop-de-mens-blijft-centraal",children:"Human in the Loop: De Mens Blijft Centraal"}),"\n",(0,r.jsx)(n.p,{children:"Human in the Loop (HITL) is cruciaal: menselijke oversight in AI-processen. In overheid betekent dit: AI adviseert of prepareert, maar mens controleert, beslist. Voorbeeld: AI genereert subsidie-advies, ambtenaar valideert \u2013 voorkomt fouten, biases, waarborgt ethiek. HITL houdt AI accountable, past bij democratische waarden en AI Act."}),"\n",(0,r.jsx)(n.h2,{id:"ethische-handelingsbevoegdheid-mens-centraal",children:"Ethische Handelingsbevoegdheid: Mens Centraal"}),"\n",(0,r.jsx)(n.p,{children:"Techniek alleen volstaat niet \u2013 ethiek is key. Verantwoord: AI versnelt werk, voorkomt tikfouten of vergeten checks, maakt informatie toegankelijker \u2013 mens eindverantwoordelijk."}),"\n",(0,r.jsx)(n.p,{children:'Problematisch: AI vervangt oordeel, discrimineert (zelfs als "statistisch klopt"), cre\xebert machteloosheid.'}),"\n",(0,r.jsx)(n.p,{children:"Voorbeeld SyRI: verboden door black box, discriminatie-risico, disproportioneel. Les: transparant, uitlegbaar, proportioneel."}),"\n",(0,r.jsx)(n.p,{children:"Kaders: transparantie (burger weet AI-gebruik), proportionaliteit, menselijke controle, non-discriminatie (bias-testing)."}),"\n",(0,r.jsx)(n.h2,{id:"kritische-vraag-vergroten-we-de-kloof-als-we-ai-niet-inzetten",children:"Kritische Vraag: Vergroten We de Kloof Als We AI Niet Inzetten?"}),"\n",(0,r.jsx)(n.p,{children:"Als we AI niet inzetten om burgers door regeldoolhoven te helpen, profiteren alleen digitaal vaardigen \u2013 vergroten we de kloof niet?"}),"\n",(0,r.jsx)(n.p,{children:"Antwoord: ja, daarom w\xe9l inzetten \u2013 verantwoord, binnen kaders, voor iedereen toegankelijk."}),"\n",(0,r.jsx)(n.h2,{id:"conclusie",children:"Conclusie"}),"\n",(0,r.jsx)(n.p,{children:"Goed ingerichte AI vergroot bestaanszekerheid, niet bureaucratie. Praktische waarborgen, juridische kaders (bevoegdheid, transparantie), ethische grenzen (mens beslist, non-discriminatie)."}),"\n",(0,r.jsx)(n.p,{children:"AI voorbereidt en adviseert. Mens beslist."}),"\n",(0,r.jsx)(n.p,{children:"Met waarborgen maken we overheid toegankelijker, effectiever, menselijker \u2013 zonder concessies aan veiligheid, privacy, democratie."}),"\n",(0,r.jsx)(n.h2,{id:"bronnen",children:"Bronnen"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n",(0,r.jsx)(n.li,{}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(g,{...e})}):g(e)}}}]);